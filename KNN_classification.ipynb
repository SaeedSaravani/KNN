{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum score of euclidean = 98.1% with k=1\n",
      "maximum score of manhattan = 97.2% with k=2\n",
      "maximum score of chebyshev = 96.7% with k=1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def predict(X, y, desired_X, n_neighbors, metric, weights, do_normalize=False):\n",
    "    if isinstance(X, pd.core.series.Series):\n",
    "        X = X.to_numpy()\n",
    "\n",
    "    if isinstance(y, pd.core.series.Series):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    y = y.reshape(-1, 1)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "\n",
    "    if isinstance(desired_X, pd.core.series.Series):\n",
    "        desired_X = desired_X.to_numpy()\n",
    "\n",
    "    if desired_X.ndim == 1:\n",
    "        desired_X = desired_X.reshape(-1, 1)\n",
    "\n",
    "    if do_normalize:\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0) + 1e-7\n",
    "        desired_X = (desired_X - X_mean) / X_std\n",
    "\n",
    "    distances = compute_distances(X, metric, desired_X)\n",
    "\n",
    "    arg_sorted_labels = np.argsort(distances, axis=1)\n",
    "    knn_weights = np.power(np.sort(distances, axis=1)[:, :n_neighbors] + 1e-7, -1)\n",
    "    knn_labels = y[arg_sorted_labels].reshape(arg_sorted_labels.shape)[:, :n_neighbors]\n",
    "\n",
    "    if weights is 'uniform':\n",
    "        unique_labels, indices = np.unique(knn_labels, return_inverse=True)\n",
    "        indices = indices.reshape(knn_labels.shape)\n",
    "        num_per_label = np.apply_along_axis(np.bincount, 1, indices, None, minlength=np.max(indices) + 1)\n",
    "        predictions = unique_labels[np.argmax(num_per_label, axis=1)]\n",
    "\n",
    "    if weights is 'distance':\n",
    "        knn_sorted_labels = np.zeros(knn_weights.shape, dtype=y.dtype)\n",
    "        for row, (weights, labels) in enumerate(zip(knn_weights, knn_labels)):\n",
    "            u_labels, indices = np.unique(labels, return_inverse=True)\n",
    "            for indice, u_label in enumerate(u_labels):\n",
    "                weights[indices == indice] = weights[indices == indice].sum()\n",
    "            labels = u_labels[indices[np.argsort(weights)]]\n",
    "            knn_sorted_labels[row] = labels\n",
    "        predictions = knn_sorted_labels[:, n_neighbors - 1]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def compute_distances(X, metric, desired_X):\n",
    "    if isinstance(X, pd.core.series.Series):\n",
    "        X = X.to_numpy()\n",
    "    along_axis_diffs = desired_X[:, np.newaxis, :] - X\n",
    "    if metric is 'euclidean':\n",
    "        distances = np.power(np.power(np.abs(along_axis_diffs), 2).sum(axis=-1), 1 / 2)\n",
    "    if metric is 'manhattan':\n",
    "        distances = np.abs(along_axis_diffs).sum(axis=-1)\n",
    "    if metric is 'chebyshev':\n",
    "        distances = np.abs(along_axis_diffs).max(axis=-1)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data\n",
    "target = digits.target\n",
    "\n",
    "X_train = data[:int(np.floor(data.shape[0] * 6 / 10))]\n",
    "X_eval = data[int(np.floor(data.shape[0] * 6 / 10)):int(np.floor(data.shape[0] * 8 / 10))]\n",
    "X_test = data[int(np.floor(data.shape[0] * 8 / 10)):]\n",
    "\n",
    "y_train = target[:int(np.floor(data.shape[0] * 6 / 10))]\n",
    "y_eval = target[int(np.floor(data.shape[0] * 6 / 10)):int(np.floor(data.shape[0] * 8 / 10))]\n",
    "y_test = target[int(np.floor(data.shape[0] * 8 / 10)):]\n",
    "\n",
    "\n",
    "def confusion_df(grand_truth, predicts):\n",
    "    df = pd.DataFrame({'predict': predicts, 'actual': grand_truth, 'cnt': 1})\n",
    "    return pd.crosstab(df.actual, df.predict)\n",
    "\n",
    "\n",
    "def compute_score(grand_truth, predicts):\n",
    "    corr = (grand_truth == predicts).sum()\n",
    "    all_ = y_eval.shape[0]\n",
    "    return corr / all_ * 100\n",
    "\n",
    "\n",
    "n_neighbors = np.asarray(range(1, 10))\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "weights = ['uniform', 'distances']\n",
    "normalize = [False, True]\n",
    "\n",
    "scores_euclidean = np.zeros((len(n_neighbors)))\n",
    "scores_manhattan = np.zeros((len(n_neighbors)))\n",
    "scores_chebyshev = np.zeros((len(n_neighbors)))\n",
    "\n",
    "for metric in metrics:\n",
    "    for n_neighbor in n_neighbors:\n",
    "        predicts = predict(X_train, y_train, X_eval, n_neighbors=n_neighbor, metric=metric,\n",
    "                           weights='distance', do_normalize=False)\n",
    "        score = compute_score(y_eval, predicts)\n",
    "        if metric is 'euclidean':\n",
    "            scores_euclidean[n_neighbor - 1] = score\n",
    "        elif metric is 'manhattan':\n",
    "            scores_manhattan[n_neighbor - 1] = score\n",
    "        else:\n",
    "            scores_chebyshev[n_neighbor - 1] = score\n",
    "\n",
    "\n",
    "print(f'maximum score of euclidean = {np.max(scores_euclidean):.1f}% \\\n",
    "with k={n_neighbors[np.argmax(scores_minkowski)]}')\n",
    "print(f'maximum score of manhattan = {np.max(scores_manhattan):.1f}% \\\n",
    "with k={n_neighbors[np.argmax(scores_manhattan)]}')\n",
    "print(f'maximum score of chebyshev = {np.max(scores_chebyshev):.1f}% \\\n",
    "with k={n_neighbors[np.argmax(scores_chebyshev)]}')\n",
    "\n",
    "predicts = predict(X_test, y_train, X_eval, n_neighbors=1, metric='euclidean',\n",
    "                   weights='distance', do_normalize=False)\n",
    "score = compute_score(y_eval, predicts)\n",
    "cdf = confusion_df(y_eval, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
